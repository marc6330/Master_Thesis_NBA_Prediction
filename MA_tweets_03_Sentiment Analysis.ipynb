{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Marc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloads\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "start = time.process_time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "\n",
    "def load_data(team, number):\n",
    "    df = pd.read_excel('C:/Users/Marc/Dropbox/06_ESCP/01_Uni/06_MA Thesis/04_Code/02_Output/02_Tweets/'+ team + '/02_Tweets/01_Final/' + team + '_final_tweets.xlsx', nrows = number)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# List of contractions to replace cleaing the data\n",
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"pt\": \"point\",\n",
    "\"fgs\" : \"fieldgoals\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "} # List of contracti"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def cleaning_tweets_tb(df_short):\n",
    "    # Clearing Text (getting rid of RT = Retweets, punctuation and more)\n",
    "    df_try = pd.DataFrame(df_short.full_text)\n",
    "    df_try['before'] = df_try.full_text\n",
    "\n",
    "    # Create functions that clean the text (right order)\n",
    "    add_space_after_period = lambda x : re.sub(\"\\.\", \". \", x)\n",
    "    harmonize_apo = lambda x : re.sub(\"â€™\", \"'\", x)\n",
    "    remove_links = lambda x : re.sub('(https?:\\/\\/[-\\/\\da-z\\.\\sA-Z0-9]+)', '', x) #remove link - always last in a tweet\n",
    "    remove_rt = lambda x : re.sub('(RT\\s@[\\w]+:)',\" \", x) # for  # for when it starts with a RT\n",
    "    remove_at = lambda x: re.sub(\"(@[\\w]+)\", \"\", x) #Remove where there are @ someting\n",
    "    remove_punctuation = lambda x: re.sub(\"[^\\w]\", ' ', x) # remove all other punctuation (that are not words) --> also emojis\n",
    "    remove_numbers = lambda x: re.sub('[0-9]', '', x)\n",
    "    remove_amp = lambda x: re.sub(\"(amp[\\s]+)\", \"\", x) #Remove where there are amp   strings\n",
    "    remove_double_space = lambda x: re.sub('[\\s]+', ' ', x)\n",
    "    lower_case = lambda x: x.lower()\n",
    "\n",
    "    # Add space after periods, to make sure words are seperate (... .Let's... -> ... .Let us)\n",
    "    df_try['full_text'] = df_try.full_text.map(add_space_after_period).map(harmonize_apo)\n",
    "\n",
    "    # expand contracted words let's -> let us\n",
    "    for tweet_index in range(len(df_try)):\n",
    "        for word in df_try.full_text.iloc[tweet_index].split():\n",
    "            if word.lower() in contractions:\n",
    "                df_try.full_text.iloc[tweet_index] = df_try.full_text.iloc[tweet_index].replace(word, contractions[word.lower()])\n",
    "\n",
    "    # Apply the removal from above\n",
    "    df_try['full_text'] = df_try.full_text.map(remove_links).map(remove_rt).map(remove_at).map(remove_punctuation).map(remove_double_space).map(remove_numbers).map(remove_amp).apply(lower_case)\n",
    "\n",
    "    # rename columns\n",
    "    df_try = df_try.rename(columns = {'full_text': 'cleaned'})\n",
    "\n",
    "    return df_try"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def preprocess_data(clean_data):\n",
    "     #classify as string\n",
    "     data = clean_data['cleaned'].astype(str)\n",
    "\n",
    "    # iniate tokenizer and lemmatizer\n",
    "     lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "     w_tokenizer =  TweetTokenizer()\n",
    "\n",
    "    # Function to lemmatize and tokenize at the same time\n",
    "     def lemmatize_text(text):\n",
    "        return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "\n",
    "    # Function to remove punctuation and create list of words\n",
    "     def remove_empty_spaces(words):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "          new_word = word\n",
    "          if new_word != '':\n",
    "             new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "     words = data.apply(lemmatize_text)\n",
    "     words = words.apply(remove_empty_spaces)\n",
    "\n",
    "\n",
    "     return pd.DataFrame(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def remove_stopwords(df_try):\n",
    "    # Remove Stopwords and other unwanted words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df_try['token_lemma'] = df_try['token_lemma'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "\n",
    "    # Create list to remove words associated to city, players and teams names\n",
    "    nba_teams = [\"token_lemma\", \"u\", \"buck\",\"v\",\"boston\",\"celtic\",\"warrior\", \"staple\", \"nba\", \"nbapacific\", \"jaylen\", \"brown\",\"knicks\",\"miami\",\"ny\",\"heat\",\"losangeleslakers\", \"knick\", \"lakersofficial\", \"losangeles\", \"lalakers\", \"b\", \"angeles\", \"los\", \"lakersbasketball\",\"la\", \"lakers\", \"Atlanta Hawks\", \"hawk\",\"celtic\",\"Boston Celtics\",\"net\",\"Brooklyn Nets\",\"hornet\",\"Charlotte Hornets\",\"bulls\",\"Chicago Bulls\",\"cavalier\",\"Cleveland Cavaliers\",\"maverick\",\"Dallas Mavericks\",\"nugget\",\"Denver Nuggets\",\"piston\",\"Detroit Pistons\",\"warrior\",\"Golden State Warriors\",\"rocket\",\"Houston Rockets\",\"pacer\",\"Indiana Pacers\",\"clippers\",\"LA Clippers\",\"LA Lakers\",\"grizzlie\", \"Memphis Grizzlies\",\"heat\",\"Miami Heat\",\"Milwaukee Bucks\", \"bucks\",\"timberwolve\",\"Minnesota Timberwolves\",\"pelican\",\"New Orleans Pelicans\",\"kinck\",\"New York Knicks\",\"Oklahoma City Thunder\", \"thunder\",\"Orlando Magic\",\"Philadelphia Sixers\", \"sixer\",\"Phoenix Suns\", \"sun\",\"Portland Trail Blazers\", \"trail\", \"blazer\",\"Sacramento Kings\", \"king\",\"NaN\",\"San Antonio Spurs\", \"spur\",\"Toronto Raptors\", \"raptor\",\"Utah Jazz\", \"jazz\",\"Washington Wizards\", \"wizard\"]\n",
    "    [x.lower() for x in nba_teams]\n",
    "\n",
    "\n",
    "    df_try['token_lemma'] = df_try['token_lemma'].apply(lambda x: [item for item in x if item not in nba_teams])\n",
    "\n",
    "\n",
    "    return df_try"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Load Data\n",
    "team = 'CHB'\n",
    "data = load_data(team, 224086)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "data.full_text = data.full_text.astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "#Clean Data\n",
    "cleaned_data = cleaning_tweets_tb(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Run function\n",
    "preprocessed_tweets = preprocess_data(cleaned_data)\n",
    "\n",
    "# Append tokenized and lemmatized words back\n",
    "cleaned_data['token_lemma'] = preprocessed_tweets\n",
    "\n",
    "# Remove stopwords and other unwanted names such as teams or players\n",
    "final_df = pd.concat([remove_stopwords(cleaned_data),data['created_at']], axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  cleaned  \\\n0       kawhi chili leonard nba leader   ppg steph cur...   \n1        two years ago i started tweeting about chicag...   \n2       since the trade deadline changed the bulls ros...   \n3        chicago bulls forward thad young said on the ...   \n4        the most unbelievable thing about the harry p...   \n...                                                   ...   \n224081   mark my words he about to be an all star and ...   \n224082  new episode a look at the    race in the west ...   \n224083   lakers survive with no lebron no true pg agai...   \n224084   and a shout out to the taylor swift sub secti...   \n224085                          help me ratio the lakers    \n\n                                                   before  \\\n0       Kawhi Chili' Leonard NBA Leader: 26.0 PPG Step...   \n1       RT @UKChicagoBulls: Two years ago I started tw...   \n2       Since the trade deadline changed the Bulls ros...   \n3       RT @ChiSportUpdates: Chicago Bulls forward Tha...   \n4       RT @HeathWParker: The most unbelievable thing ...   \n...                                                   ...   \n224081  @Lakers Mark my words he about to be an all-st...   \n224082  New Episode - A look at the 5-6-7 race in the ...   \n224083  RT @Sedano: Lakers survive with no LeBron, no ...   \n224084  RT @xKENNANx: and a shout out to the Taylor Sw...   \n224085   Help me ratio the Lakers https://t.co/btC0ZsXfKa   \n\n                                              token_lemma          created_at  \n0       [kawhi, chili, leonard, leader, ppg, steph, cu... 2021-04-13 17:04:40  \n1       [two, year, ago, started, tweeting, chicago, b... 2021-04-13 17:02:36  \n2       [since, trade, deadline, changed, bull, roster... 2021-04-13 17:00:26  \n3       [chicago, bull, forward, thad, young, said, bu... 2021-04-13 16:59:01  \n4       [unbelievable, thing, harry, potter, series, t... 2021-04-13 16:44:13  \n...                                                   ...                 ...  \n224081                         [mark, word, star, called] 2021-05-12 15:16:10  \n224082  [new, episode, look, race, west, try, hold, ca... 2021-05-12 15:16:09  \n224083  [survive, lebron, true, pg, best, league, th, ... 2021-05-12 15:16:09  \n224084  [shout, taylor, swift, sub, section, twitter, ... 2021-05-12 15:16:05  \n224085                                      [help, ratio] 2021-05-12 15:15:46  \n\n[224086 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cleaned</th>\n      <th>before</th>\n      <th>token_lemma</th>\n      <th>created_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>kawhi chili leonard nba leader   ppg steph cur...</td>\n      <td>Kawhi Chili' Leonard NBA Leader: 26.0 PPG Step...</td>\n      <td>[kawhi, chili, leonard, leader, ppg, steph, cu...</td>\n      <td>2021-04-13 17:04:40</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>two years ago i started tweeting about chicag...</td>\n      <td>RT @UKChicagoBulls: Two years ago I started tw...</td>\n      <td>[two, year, ago, started, tweeting, chicago, b...</td>\n      <td>2021-04-13 17:02:36</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>since the trade deadline changed the bulls ros...</td>\n      <td>Since the trade deadline changed the Bulls ros...</td>\n      <td>[since, trade, deadline, changed, bull, roster...</td>\n      <td>2021-04-13 17:00:26</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>chicago bulls forward thad young said on the ...</td>\n      <td>RT @ChiSportUpdates: Chicago Bulls forward Tha...</td>\n      <td>[chicago, bull, forward, thad, young, said, bu...</td>\n      <td>2021-04-13 16:59:01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the most unbelievable thing about the harry p...</td>\n      <td>RT @HeathWParker: The most unbelievable thing ...</td>\n      <td>[unbelievable, thing, harry, potter, series, t...</td>\n      <td>2021-04-13 16:44:13</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>224081</th>\n      <td>mark my words he about to be an all star and ...</td>\n      <td>@Lakers Mark my words he about to be an all-st...</td>\n      <td>[mark, word, star, called]</td>\n      <td>2021-05-12 15:16:10</td>\n    </tr>\n    <tr>\n      <th>224082</th>\n      <td>new episode a look at the    race in the west ...</td>\n      <td>New Episode - A look at the 5-6-7 race in the ...</td>\n      <td>[new, episode, look, race, west, try, hold, ca...</td>\n      <td>2021-05-12 15:16:09</td>\n    </tr>\n    <tr>\n      <th>224083</th>\n      <td>lakers survive with no lebron no true pg agai...</td>\n      <td>RT @Sedano: Lakers survive with no LeBron, no ...</td>\n      <td>[survive, lebron, true, pg, best, league, th, ...</td>\n      <td>2021-05-12 15:16:09</td>\n    </tr>\n    <tr>\n      <th>224084</th>\n      <td>and a shout out to the taylor swift sub secti...</td>\n      <td>RT @xKENNANx: and a shout out to the Taylor Sw...</td>\n      <td>[shout, taylor, swift, sub, section, twitter, ...</td>\n      <td>2021-05-12 15:16:05</td>\n    </tr>\n    <tr>\n      <th>224085</th>\n      <td>help me ratio the lakers</td>\n      <td>Help me ratio the Lakers https://t.co/btC0ZsXfKa</td>\n      <td>[help, ratio]</td>\n      <td>2021-05-12 15:15:46</td>\n    </tr>\n  </tbody>\n</table>\n<p>224086 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# Worldcloud\n",
    "def create_wordcloud(final_df):\n",
    "    from wordcloud import WordCloud\n",
    "\n",
    "    #WordCloud\n",
    "    wc = WordCloud(background_color=\"white\", max_words=3000,repeat=True)\n",
    "    wc.generate(str(final_df['token_lemma']))\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TF-idf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# For tweets there are not title, hence no difference weights assigned\n",
    "\n",
    "def create_df(final_df):\n",
    "    DF = {}\n",
    "\n",
    "    for i in range(len(final_df)):\n",
    "\n",
    "        tokens = final_df['token_lemma'][i]\n",
    "\n",
    "        for w in tokens:\n",
    "            try:\n",
    "                DF[w].add(i)\n",
    "            except:\n",
    "                DF[w] = {i}\n",
    "\n",
    "    for i in DF:\n",
    "        DF[i] = len(DF[i])\n",
    "\n",
    "    return DF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#helper function for calc tf_idf\n",
    "def return_word_freq(word):\n",
    "    f = 0\n",
    "    try:\n",
    "        f = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Calculating tf-idf\n",
    "#tf = Term frequencyy --> depends on the tweet\n",
    "#idf = takes whole corpus into account (all tweets)\n",
    "\n",
    "def calc_tf_idf(final_df):\n",
    "    tf_idf = {}\n",
    "\n",
    "    for i in range(len(final_df)):\n",
    "\n",
    "        tokens = final_df['token_lemma'][i]\n",
    "        counter = Counter(tokens)\n",
    "        words_count = len(final_df['token_lemma'][i])\n",
    "\n",
    "        for token in np.unique(tokens):\n",
    "\n",
    "            tf = counter[token]/words_count\n",
    "            df = return_word_freq(token)\n",
    "            idf = np.log((len(final_df)+1)/(df+1)) #--> by adding one in the nummerator no negative values are possible\n",
    "\n",
    "            tf_idf[token] = df*idf\n",
    "\n",
    "    return tf_idf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DF = create_df(final_df)\n",
    "tf_idf = calc_tf_idf(final_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Vectorising\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(tf_idf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not done, the transform might work, the tokenization and everything up to the td-idf scores works. However the cosine similiratity is a bit tricky to be calculated.\n",
    "The problem espeically starts with comparing a new corpus' similarity to an exisitng"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TextBlob and Vader\n",
    "Vader: https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Add polarity, neg, pos... to the tweets\n",
    "\n",
    "final_df[['TB_polarity', 'TB_subjectivity']] = final_df['cleaned'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "\n",
    "for index, row in final_df['cleaned'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    final_df.loc[index, 'Va_neg'] = neg\n",
    "    final_df.loc[index, 'Va_neu'] = neu\n",
    "    final_df.loc[index, 'Va_pos'] = pos\n",
    "    final_df.loc[index, 'Va_compound'] = comp\n",
    "    if neg > pos:\n",
    "        final_df.loc[index, 'Va_sentiment'] = 'negative'\n",
    "    elif pos > neg:\n",
    "        final_df.loc[index, 'Va_sentiment'] = 'positive'\n",
    "    else:\n",
    "        final_df.loc[index, 'Va_sentiment'] = 'neutral'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# Use not cleanded dataset (cleaning now done by TB and Vader (Vader can read Emojis))\n",
    "final_df[['_before_TB_polarity', '_before_TB_subjectivity']] = final_df['before'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "\n",
    "for index, row in final_df['before'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    final_df.loc[index, '_before_Va_neg'] = neg\n",
    "    final_df.loc[index, '_before_Va_neu'] = neu\n",
    "    final_df.loc[index, '_before_Va_pos'] = pos\n",
    "    final_df.loc[index, '_before_Va_compound'] = comp\n",
    "    if neg > pos:\n",
    "        final_df.loc[index, '_before_Va_sentiment'] = 'negative'\n",
    "    elif pos > neg:\n",
    "        final_df.loc[index, '_before_Va_sentiment'] = 'positive'\n",
    "    else:\n",
    "        final_df.loc[index, '_before_Va_sentiment'] = 'neutral'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "final_df.to_csv('C:/Users/Marc/Dropbox/06_ESCP/01_Uni/06_MA Thesis/04_Code/02_Output/02_Tweets/'+ team +'/02_Tweets/01_Final/'+ team +'_sentiment_analized.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6042.25\n"
     ]
    }
   ],
   "source": [
    "print(time.process_time() - start)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instruction:\n",
    "- Change team name in 252 and run\n",
    "- Check if that is the correct folder name\n",
    "    - Done for LAL\n",
    "    - GSW\n",
    "    - BOC\n",
    "    - MIH\n",
    "    - CHB\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}